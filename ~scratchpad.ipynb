{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\content-assistant\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://huggingface.co/docs/huggingface_hub/v0.20.2/en/package_reference/inference_client#huggingface_hub.InferenceClient\n",
    "\n",
    ":params:\n",
    ":model (str, optional) — The model to run inference with. Can be a model id hosted on the Hugging Face Hub, e.g. bigcode/starcoder or a URL to a deployed Inference Endpoint. Defaults to None, in which case a recommended model is automatically selected for the task.\n",
    "\n",
    ":token (str, optional) — Hugging Face token. Will default to the locally saved token. Pass token=False if you don’t want to send your token to the server.\n",
    "\n",
    ":timeout (float, optional) — The maximum number of seconds to wait for a response from the server. Loading a new model in Inference API can take up to several minutes. Defaults to None, meaning it will loop until the server is available.\n",
    "\n",
    ":headers (Dict[str, str], optional) — Additional headers to send to the server. By default only the authorization and user-agent headers are sent. Values in this dictionary will override the default values.\n",
    ":cookies (Dict[str, str], optional) — Additional cookies to send to the server.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "default_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "client = InferenceClient(\n",
    "    model = default_model,\n",
    "    token = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://huggingface.co/docs/huggingface_hub/v0.20.2/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient.conversational\n",
    "\n",
    ":params:\n",
    ":text (str) — The last input from the user in the conversation.\n",
    "\n",
    ":generated_responses (List[str], optional) — A list of strings corresponding to the earlier replies from the model. Defaults to None.\n",
    "\n",
    ":past_user_inputs (List[str], optional) — A list of strings corresponding to the earlier replies from the user. Should be the same length as generated_responses. Defaults to None.\n",
    "\n",
    ":parameters (Dict[str, Any], optional) — Additional parameters for the conversational task. Defaults to None. For more details about the available parameters, please refer to this page\n",
    "\n",
    ":model (str, optional) — The model to use for the conversational task. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. If not provided, the default recommended conversational model will be used. Defaults to None.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def chat_engine():\n",
    "    chat = client.conversational(\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "research agent\n",
    "text-classification > action\n",
    "\n",
    "https://huggingface.co/docs/huggingface_hub/v0.20.2/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_classification\n",
    "\n",
    ":params:\n",
    ":text (str) — A string to be classified.\n",
    "\n",
    ":model (str, optional) — The model to use for the text classification task. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. If not provided, the default recommended text classification model will be used. Defaults to None.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "client.text_classification(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://huggingface.co/docs/huggingface_hub/v0.20.2/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation\n",
    "\n",
    ":params:\n",
    ":prompt (str) — Input text.\n",
    "\n",
    ":details (bool, optional) — By default, text_generation returns a string. Pass details=True if you want a \n",
    "\n",
    ":detailed output (tokens, probabilities, seed, finish reason, etc.). Only available for models running on with the text-generation-inference backend.\n",
    "\n",
    ":stream (bool, optional) — By default, text_generation returns the full generated text. Pass stream=True if you want a stream of tokens to be returned. Only available for models running on with the text-generation-inference backend.\n",
    "\n",
    ":model (str, optional) — The model to use for inference. Can be a model ID hosted on the Hugging Face Hub or a URL to a deployed Inference Endpoint. This parameter overrides the model defined at the instance level. Defaults to None.\n",
    "\n",
    ":do_sample (bool) — Activate logits sampling\n",
    "\n",
    ":max_new_tokens (int) — Maximum number of generated tokens\n",
    "\n",
    ":best_of (int) — Generate best_of sequences and return the one if the highest token logprobs\n",
    "\n",
    ":repetition_penalty (float) — The parameter for repetition penalty. 1.0 means no penalty. See this paper for more details.\n",
    "\n",
    ":return_full_text (bool) — Whether to prepend the prompt to the generated text\n",
    "\n",
    ":seed (int) — Random sampling seed\n",
    "\n",
    ":stop_sequences (List[str]) — Stop generating tokens if a member of stop_sequences is generated\n",
    "\n",
    ":temperature (float) — The value used to module the logits distribution.\n",
    "\n",
    ":top_k (int) — The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "\n",
    ":top_p (float) — If set to < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "\n",
    ":truncate (int) — Truncate inputs tokens to the given size\n",
    "\n",
    ":typical_p (float) — Typical Decoding mass See Typical Decoding for Natural Language Generation for more information\n",
    "\n",
    ":watermark (bool) — Watermarking with A Watermark for Large Language Models\n",
    "\n",
    ":decoder_input_details (bool) — Return the decoder input token logprobs and ids. You must set details=True as well for it to be taken into account. Defaults to False.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "client.text_generation(\n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
